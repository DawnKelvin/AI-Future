{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Ig83lUA0ikaqOILgNEG67u1X7L1g6Kgo",
      "authorship_tag": "ABX9TyPd2QV/1yGIJ54cR8HLDtGU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DawnKelvin/AI-Future/blob/main/RecyclableImgClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S25GsMv9-Pj",
        "outputId": "40756efd-02e5-472d-a00b-04176bc1296f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Environment Setup\n",
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset: TrashNet\n",
        "A popular open-source dataset with ~2,500 labeled images (glass, paper, cardboard, plastic, metal, trash). Created by Stanford students, it's widely used for lightweight waste classification. Includes resized, ready-to-use versions.\n",
        "\n",
        "Data: https://github.com/garythung/trashnet.git"
      ],
      "metadata": {
        "id": "-zrVFjMFUgPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess dataset\n",
        "# Define image size and batch size\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Assuming you uploaded dataset in folders: plastic/, glass/, paper/, etc.\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_data = datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/dataset-resized',  # Adjust path\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_data = datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/dataset-resized',\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s33e0LoF_es6",
        "outputId": "a04a4fdc-5efc-4ced-9af7-4289ae08f1ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2029 images belonging to 6 classes.\n",
            "Found 505 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define lightweight CNN\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=IMG_SIZE + (3,)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(train_data.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "xj19jsE9Gyeq",
        "outputId": "f2243d78-c2a9-452f-dabc-eac934d50aa1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │           \u001b[38;5;34m448\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m802,880\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m390\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">802,880</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m826,854\u001b[0m (3.15 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">826,854</span> (3.15 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m826,854\u001b[0m (3.15 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">826,854</span> (3.15 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=5\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBclxIZlGyjJ",
        "outputId": "5be25fc3-9bb2-4c11-ed14-5b38ba397701"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.3897 - loss: 1.4545"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m639s\u001b[0m 10s/step - accuracy: 0.3902 - loss: 1.4535 - val_accuracy: 0.4099 - val_loss: 1.4847\n",
            "Epoch 2/5\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 848ms/step - accuracy: 0.5103 - loss: 1.2166 - val_accuracy: 0.4475 - val_loss: 1.4004\n",
            "Epoch 3/5\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 795ms/step - accuracy: 0.5454 - loss: 1.1237 - val_accuracy: 0.4495 - val_loss: 1.3709\n",
            "Epoch 4/5\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 790ms/step - accuracy: 0.6429 - loss: 0.9073 - val_accuracy: 0.4693 - val_loss: 1.3833\n",
            "Epoch 5/5\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 834ms/step - accuracy: 0.6809 - loss: 0.8114 - val_accuracy: 0.4792 - val_loss: 1.3175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u0e5JdFLKTEV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "054bc829"
      },
      "source": [
        "This code defines and compiles a lightweight Convolutional Neural Network (CNN) using TensorFlow's Keras API.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "1.  **`model = tf.keras.models.Sequential([...])`**: This line initializes a sequential model, which means the layers are stacked one after another.\n",
        "\n",
        "2.  **Convolutional Layers (`Conv2D`)**:\n",
        "    *   `tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=IMG_SIZE + (3,))`: This is the first convolutional layer.\n",
        "        *   `16`: The number of filters (feature detectors) in this layer.\n",
        "        *   `(3,3)`: The size of the convolution kernel (the window that slides over the image).\n",
        "        *   `activation='relu'`: The ReLU (Rectified Linear Unit) activation function is applied to the output of the convolution.\n",
        "        *   `input_shape=IMG_SIZE + (3,)`: Specifies the expected input shape. `IMG_SIZE` is a tuple `(height, width)`, and `(3,)` adds the color channels (for RGB images).\n",
        "    *   `tf.keras.layers.Conv2D(32, (3,3), activation='relu')`: A second convolutional layer with 32 filters.\n",
        "    *   `tf.keras.layers.Conv2D(64, (3,3), activation='relu')`: A third convolutional layer with 64 filters.\n",
        "\n",
        "3.  **MaxPooling Layers (`MaxPooling2D`)**:\n",
        "    *   `tf.keras.layers.MaxPooling2D(2,2)`: These layers reduce the spatial dimensions (width and height) of the feature maps. A pool size of `(2,2)` means the layer takes the maximum value from each 2x2 window.\n",
        "\n",
        "4.  **Flatten Layer (`Flatten`)**:\n",
        "    *   `tf.keras.layers.Flatten()`: This layer converts the 2D feature maps into a 1D vector, which is necessary to feed the data into the dense layers.\n",
        "\n",
        "5.  **Dense Layers (`Dense`)**:\n",
        "    *   `tf.keras.layers.Dense(64, activation='relu')`: A fully connected layer with 64 neurons and ReLU activation.\n",
        "    *   `tf.keras.layers.Dense(train_data.num_classes, activation='softmax')`: The output layer. The number of neurons is equal to the number of classes in your dataset (`train_data.num_classes`), and the `softmax` activation function outputs probabilities for each class, summing up to 1.\n",
        "\n",
        "6.  **Compile the Model**:\n",
        "    *   `model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])`: This configures the model for training.\n",
        "        *   `optimizer='adam'`: The optimization algorithm used to update the model's weights during training.\n",
        "        *   `loss='categorical_crossentropy'`: The loss function suitable for multi-class classification problems with one-hot encoded labels.\n",
        "        *   `metrics=['accuracy']`: The metric used to evaluate the model's performance during training and evaluation.\n",
        "\n",
        "7.  **Model Summary**:\n",
        "    *   `model.summary()`: Prints a summary of the model's architecture, including the output shape and the number of parameters for each layer.\n",
        "\n",
        "This code defines the structure of the neural network that will be trained to classify images of different recyclable materials."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model and save\n",
        "model.evaluate(val_data)\n",
        "model.save('recyclable_classifier.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG2rICGpHY2C",
        "outputId": "341b8ae1-ef52-466c-ae88-9d853e184374"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 285ms/step - accuracy: 0.4749 - loss: 1.3159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to TensorFlow Lite for Rasperry Pi\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('recyclable_classifier.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn6tFFuAHhsu",
        "outputId": "02bf545b-7925-4272-bcf2-2c8602f9a871"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpclhjhb21'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  139092497510288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139092497511440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139092497509712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139092497507984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139092497510864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139092497510480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139092497508176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139092497508560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139092497512976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139092497513936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D2YxYG57HxpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test on sample dataset"
      ],
      "metadata": {
        "id": "4voPOubUBGv6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c994c655"
      },
      "source": [
        "# Task\n",
        "Convert the model to TensorFlow Lite and test it on a sample dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65640490"
      },
      "source": [
        "## Load the tflite model\n",
        "\n",
        "### Subtask:\n",
        "Load the saved `.tflite` model into a TensorFlow Lite Interpreter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81569f4b"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary class and load the TFLite model using the Interpreter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a08d0b1"
      },
      "source": [
        "from tensorflow.lite.python.interpreter import Interpreter\n",
        "\n",
        "# Load the TFLite model and allocate tensors.\n",
        "interpreter = Interpreter(model_path='recyclable_classifier.tflite')\n",
        "interpreter.allocate_tensors()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24b474b0"
      },
      "source": [
        "## Prepare sample data\n",
        "\n",
        "### Subtask:\n",
        "Load and preprocess a sample image or a batch of images to match the input requirements of the TFLite model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e31760a"
      },
      "source": [
        "**Reasoning**:\n",
        "Load and preprocess a sample image to match the input requirements of the TFLite model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ee9176e",
        "outputId": "c913fa3f-6f2a-48cf-c583-51346b34c5cb"
      },
      "source": [
        "# Choose a sample image file path (replace with an actual path from your dataset)\n",
        "sample_image_path = '/content/drive/MyDrive/Colab Notebooks/dataset-resized/plastic/plastic1.jpg' # Example path\n",
        "\n",
        "# Load the image\n",
        "img = tf.keras.preprocessing.image.load_img(sample_image_path, target_size=IMG_SIZE)\n",
        "\n",
        "# Convert the image to a NumPy array\n",
        "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "\n",
        "# Expand dimensions to include batch size\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "# Rescale the image\n",
        "img_array = img_array / 255.0\n",
        "\n",
        "print(\"Sample image loaded and preprocessed with shape:\", img_array.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample image loaded and preprocessed with shape: (1, 128, 128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "328148de"
      },
      "source": [
        "## Run inference\n",
        "\n",
        "### Subtask:\n",
        "Use the TFLite Interpreter to run inference on the prepared sample data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfa67a47"
      },
      "source": [
        "**Reasoning**:\n",
        "Use the TFLite interpreter to run inference on the prepared sample data by getting input and output tensor details, setting the input tensor, invoking the interpreter, and getting the output tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6194b01",
        "outputId": "70b75ab9-977e-40f5-9dec-2b96288bfbc9"
      },
      "source": [
        "# Get input and output tensor details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Set the input tensor\n",
        "interpreter.set_tensor(input_details[0]['index'], img_array)\n",
        "\n",
        "# Invoke the interpreter\n",
        "interpreter.invoke()\n",
        "\n",
        "# Get the output tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Inference completed. Output data shape:\", output_data.shape)\n",
        "print(\"Output data:\", output_data)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference completed. Output data shape: (1, 6)\n",
            "Output data: [[0.00244701 0.3235134  0.11538694 0.46402404 0.07492646 0.01970217]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1db92a3e"
      },
      "source": [
        "## Interpret the results\n",
        "\n",
        "### Subtask:\n",
        "Process the output of the inference to get the class predictions and their probabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcd5c309"
      },
      "source": [
        "**Reasoning**:\n",
        "Process the output of the inference to get the class predictions and their probabilities by getting the predicted class index using argmax, retrieving class labels, creating a list of class names, finding the predicted class name and its probability, and printing the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bacc477a",
        "outputId": "c13921f1-4986-47d4-a63a-2ac37c033f63"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the index of the highest probability\n",
        "predicted_class_index = np.argmax(output_data)\n",
        "\n",
        "# Get the class labels from train_data.class_indices\n",
        "class_indices = train_data.class_indices\n",
        "\n",
        "# Create a list of class names in the correct order\n",
        "class_names = [key for key, value in sorted(class_indices.items(), key=lambda item: item[1])]\n",
        "\n",
        "# Get the predicted class name\n",
        "predicted_class_name = class_names[predicted_class_index]\n",
        "\n",
        "# Get the probability of the predicted class\n",
        "predicted_probability = output_data[0][predicted_class_index]\n",
        "\n",
        "# Print the predicted class name and its probability\n",
        "print(\"Predicted class:\", predicted_class_name)\n",
        "print(\"Predicted probability:\", predicted_probability)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: paper\n",
            "Predicted probability: 0.46402404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dceeba4c"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The TensorFlow Lite model was successfully loaded using the `Interpreter` class and `allocate_tensors()` was called to prepare it for inference.\n",
        "*   A sample image was loaded, resized to 128x128 pixels, converted to a NumPy array, expanded to include a batch dimension (resulting shape (1, 128, 128, 3)), and rescaled to the range \\[0, 1].\n",
        "*   Inference was successfully run on the prepared sample data using the TFLite interpreter. The output shape was (1, 6), indicating a batch size of 1 and 6 output values (likely class probabilities).\n",
        "*   The predicted class for the sample image was identified as \"paper\" with a probability of approximately 0.464.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   To fully test the model, run inference on a larger and more diverse sample dataset and evaluate metrics like accuracy, precision, and recall.\n",
        "*   Compare the performance of the TFLite model with the original TensorFlow model on the same test set to ensure conversion did not significantly impact accuracy.\n"
      ]
    }
  ]
}
